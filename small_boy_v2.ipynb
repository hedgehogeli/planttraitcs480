{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "771d5dce",
      "metadata": {
        "id": "771d5dce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from IPython.display import display, clear_output\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os.path\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fvmLKq39lArQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fvmLKq39lArQ",
        "outputId": "ed2d6f15-473f-41f1-ba9d-576c64a70e98"
      },
      "outputs": [],
      "source": [
        "# ! pip install -q kaggle\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gDb3bJ9plOMO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDb3bJ9plOMO",
        "outputId": "d69a2094-8b8e-4da6-fb80-6ce1c130e070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cs-480-2024-spring.zip to /content\n",
            " 99% 284M/287M [00:04<00:00, 78.0MB/s]\n",
            "100% 287M/287M [00:04<00:00, 65.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ! mkdir ~/.kaggle\n",
        "# ! cp kaggle.json ~/.kaggle/\n",
        "# ! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle competitions download -c cs-480-2024-spring\n",
        "# ! mkdir data\n",
        "# ! unzip -q cs-480-2024-spring.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "74010e76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74010e76",
        "outputId": "20a682ec-fd56-4df7-cdb4-240178339fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On CPU\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print('On GPU') if device == torch.device(\"cuda:0\") else print('On CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "87f5c100",
      "metadata": {
        "id": "87f5c100"
      },
      "outputs": [],
      "source": [
        "class Hyperparameter:\n",
        "    num_predictions: int    = 6\n",
        "\n",
        "    # hyperparams\n",
        "    batch_size: int         = 64\n",
        "    num_epochs: int         = 8\n",
        "\n",
        "    vgg_output_size: int    = 4096\n",
        "    fc_output_size: int     = 256\n",
        "\n",
        "    # bookkeeping\n",
        "    batch_report_gap: int   = 96\n",
        "    sav_model_epoch: bool   = True\n",
        "    vals_per_epoch: int     = 3\n",
        "\n",
        "    # optimizer\n",
        "    learning_rate: float    = 0.0005\n",
        "    lr_decay: float         = 0.9\n",
        "\n",
        "hp = Hyperparameter()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e2c06d",
      "metadata": {
        "id": "77e2c06d"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ed4bf1b1",
      "metadata": {
        "id": "ed4bf1b1"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_id, data, target, img_dir, transform=None):\n",
        "        self.img_id = img_id\n",
        "        self.data_frame = data\n",
        "        self.target = target\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, f\"{self.img_id[idx]}.jpeg\")\n",
        "        image = np.array(plt.imread(img_name), dtype=np.float32)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        training_data = torch.tensor(self.data_frame[idx])\n",
        "        target_data = torch.tensor(self.target[idx])\n",
        "\n",
        "        return image, training_data, target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f84a43a1",
      "metadata": {
        "id": "f84a43a1"
      },
      "outputs": [],
      "source": [
        "MEAN = [113.82422637939453/255, 114.86695861816406/255, 85.6895751953125/255]\n",
        "STD = [46.77458190917969/255, 45.75661849975586/255, 45.359466552734375/255]\n",
        "\n",
        "augment_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=MEAN, std=STD),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomResizedCrop(size=128, scale=(0.8, 1.0)),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "    ])\n",
        "\n",
        "standard_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=MEAN, std=STD)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ea3b3509",
      "metadata": {
        "id": "ea3b3509"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "TRAIN_IMG_DIR = os.path.join('data', 'train_images')\n",
        "EVAL_IMG_DIR = os.path.join('data', 'test_images')\n",
        "\n",
        "train_data = pd.read_csv(os.path.join('data', 'train.csv'))\n",
        "evaluation_data = pd.read_csv(os.path.join('data', 'test.csv'))\n",
        "\n",
        "# normalise data (convert to Z score)\n",
        "data_to_normalise = train_data[train_data.columns[1:164]]\n",
        "eval_to_normalise = evaluation_data[evaluation_data.columns[1:]]\n",
        "\n",
        "stats_df = pd.DataFrame({\n",
        "    'mean': data_to_normalise.mean(),\n",
        "    'std': data_to_normalise.std() })\n",
        "\n",
        "data_normd = (data_to_normalise - stats_df['mean']) / stats_df['std']\n",
        "eval_normd = (eval_to_normalise - stats_df['mean']) / stats_df['std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8ea2dfd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ea2dfd4",
        "outputId": "049c3693-be4e-4446-f84f-83c5511d41d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((850, 163), (150, 163), 14, 3)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shove into dataloaders\n",
        "X = np.array(data_normd, dtype=np.float32)\n",
        "Y = np.array(train_data, dtype=np.float32)[:, -6:]\n",
        "\n",
        "X = X[:1000]\n",
        "Y = Y[:1000]\n",
        "\n",
        "TRAIN_TEST_SPLIT = int(np.round(0.85 * len(X)))\n",
        "\n",
        "train_img_id = train_data['id'][:TRAIN_TEST_SPLIT]\n",
        "train_x = X[:TRAIN_TEST_SPLIT]\n",
        "train_y = Y[:TRAIN_TEST_SPLIT]\n",
        "train_dataset = CustomDataset(train_img_id, train_x, train_y, img_dir=TRAIN_IMG_DIR, transform=augment_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True)\n",
        "\n",
        "test_img_id = train_data['id'][TRAIN_TEST_SPLIT:].reset_index(drop=True)\n",
        "test_x = X[TRAIN_TEST_SPLIT:]\n",
        "test_y = Y[TRAIN_TEST_SPLIT:]\n",
        "test_dataset = CustomDataset(test_img_id, test_x, test_y, img_dir=TRAIN_IMG_DIR, transform=standard_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hp.batch_size, shuffle=False)\n",
        "\n",
        "train_x.shape, test_x.shape, len(train_dataloader), len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb891b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cb891b0",
        "outputId": "8dfc66e8-3738-482a-ed99-6df797df008e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6391, 163), (6391, 6), 100)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EVAL_X = np.array(eval_normd, dtype=np.float32)\n",
        "\n",
        "eval_img_id = evaluation_data['id']\n",
        "eval_x = np.array(EVAL_X, dtype=np.float32)\n",
        "eval_y = np.zeros((6391,6), dtype=np.float32) # empty\n",
        "eval_dataset = CustomDataset(eval_img_id, eval_x, eval_y, img_dir=EVAL_IMG_DIR, transform=standard_transform)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=hp.batch_size, shuffle=False)\n",
        "\n",
        "eval_x.shape, eval_y.shape, len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5d8239",
      "metadata": {
        "id": "4e5d8239"
      },
      "outputs": [],
      "source": [
        "# all_img_id = train_data['id']\n",
        "# full_dataset = CustomDataset(all_img_id, X, Y, img_dir=TRAIN_IMG_DIR, transform = standard_transform)\n",
        "# full_dataloader = DataLoader(full_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# mean = 0.0\n",
        "# std = 0.0\n",
        "# total_images = 0\n",
        "\n",
        "# for images, _, _ in full_dataloader:\n",
        "#     images = images.view(images.size(0), images.size(1), -1)  # Reshape to (batch_size, channels, height*width)\n",
        "#     mean += images.mean(2).sum(0)\n",
        "#     std += images.std(2).sum(0)\n",
        "#     total_images += images.size(0)\n",
        "\n",
        "# mean /= total_images\n",
        "# std /= total_images\n",
        "\n",
        "# mean.tolist()\n",
        "# std.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc3b8b58",
      "metadata": {
        "id": "bc3b8b58"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c52a5e4",
      "metadata": {
        "id": "6c52a5e4"
      },
      "outputs": [],
      "source": [
        "class VGG11(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG11, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # - Conv(003, 064, 3, 1, 1) - BatchNorm(064) - ReLU - MaxPool(2, 2)\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
        "            # - Conv(064, 128, 3, 1, 1) - BatchNorm(128) - ReLU - MaxPool(2, 2)\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
        "            # - Conv(128, 256, 3, 1, 1) - BatchNorm(256) - ReLU\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            # # - Conv(256, 256, 3, 1, 1) - BatchNorm(256) - ReLU - MaxPool(2, 2)\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
        "            # - Conv(256, 512, 3, 1, 1) - BatchNorm(512) - ReLU\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            # - Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU - MaxPool(2, 2)\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
        "            # - Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
        "            # - Conv(512, 512, 3, 1, 1) - BatchNorm(512) - ReLU - MaxPool(2, 2)\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
        "            # - FC(0512, 4096) - ReLU - Dropout(0.5)\n",
        "            nn.Flatten(),\n",
        "            # nn.Linear(512*4*4, 4096), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            # # - FC(4096, 4096) - ReLU - Dropout(0.5)\n",
        "            # nn.Linear(4096, 2048), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            # - FC(4096, out)\n",
        "            # nn.Linear(4096, hp.vgg_output_size)\n",
        "\n",
        "            nn.Linear(512*4*4, hp.vgg_output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        return self.cnn(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab816be",
      "metadata": {
        "id": "aab816be"
      },
      "outputs": [],
      "source": [
        "class FC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FC, self).__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(163, 256), nn.ReLU(inplace=True), # nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 512), nn.ReLU(inplace=True), # nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 512), nn.ReLU(inplace=True), # nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, hp.fc_output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.linear(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9e1a11",
      "metadata": {
        "id": "cf9e1a11"
      },
      "outputs": [],
      "source": [
        "class SmallBoy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallBoy, self).__init__()\n",
        "\n",
        "        self.vgg = VGG11()\n",
        "        self.fc = FC()\n",
        "\n",
        "        concat_size = hp.vgg_output_size + hp.fc_output_size\n",
        "\n",
        "        self.linear_head = nn.Sequential(\n",
        "            nn.Linear(concat_size, concat_size), nn.ReLU(inplace=True), nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(concat_size, concat_size), nn.ReLU(inplace=True), nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(concat_size, hp.num_predictions)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        img = input[0]\n",
        "        data = input[1]\n",
        "\n",
        "        vgg_result = self.vgg(img)\n",
        "        data_result = self.fc(data)\n",
        "\n",
        "        combined = torch.cat((vgg_result, data_result), dim=1)\n",
        "\n",
        "        return self.linear_head(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6075112f",
      "metadata": {
        "id": "6075112f"
      },
      "source": [
        "# Bookkeeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e4662cc",
      "metadata": {
        "id": "6e4662cc"
      },
      "outputs": [],
      "source": [
        "# bookkeeping block\n",
        "\n",
        "class bookkeeper():\n",
        "    def __init__(self, key_lst, total_batches):\n",
        "\n",
        "        # graphs\n",
        "        self.lists = {}\n",
        "        for key in key_lst:\n",
        "            self.lists[key] = {'batch': [],\n",
        "                               'value': []}\n",
        "\n",
        "        self.batch_loss_list = []\n",
        "\n",
        "        # timer\n",
        "        self.start_time = time.time()\n",
        "        self.total_batches = total_batches\n",
        "        self.local_batches_completed = 0\n",
        "        self.batches_completed = 0\n",
        "        self.epochs_completed = 0\n",
        "\n",
        "    def append(self, key, val):\n",
        "        self.lists[key]['batch'].append(self.batches_completed)\n",
        "        self.lists[key]['value'].append(val)\n",
        "\n",
        "    def init_plots():\n",
        "        plt.ion()\n",
        "\n",
        "\n",
        "    def plot(self):\n",
        "        for k in self.lists:\n",
        "            plt.figure(figsize=(10,5))\n",
        "            plt.title(k)\n",
        "            plt.xticks(np.arange(0, np.array(self.lists[k]['batch']).max()+len(train_dataloader), len(train_dataloader)))\n",
        "            plt.plot(self.lists[k]['batch'][1:], self.lists[k]['value'][1:]) #ignore 1st entry, too big\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        # plot batch loss moving average of window size 5:\n",
        "        data_series = pd.Series(self.batch_loss_list)\n",
        "        moving_average = data_series.rolling(window=10, center=False).mean()\n",
        "        plt.title('moving batch loss')\n",
        "        plt.plot(moving_average)\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def reset_timer(self):\n",
        "        self.start_time = time.time()\n",
        "        print('cur time', datetime.fromtimestamp(self.start_time).strftime(\"%H:%M:%S\"))\n",
        "\n",
        "    def elapsed_time(self):\n",
        "        elapsed = time.time() - self.start_time\n",
        "        hours, rem = divmod(elapsed, 3600)\n",
        "        minutes, seconds = divmod(rem, 60)\n",
        "        return 'ELAPSED: ' + f\"{int(hours):02}h {int(minutes):02}m {int(seconds):02}s\"\n",
        "\n",
        "    def eta(self):\n",
        "        progress = (self.batches_completed+1) / self.total_batches\n",
        "        elapsed = time.time() - self.start_time\n",
        "        est_total_time = elapsed / progress\n",
        "        eta = self.start_time + est_total_time\n",
        "        formatted_eta = datetime.fromtimestamp(eta).strftime(\"%H:%M:%S\")\n",
        "        return 'ETA: ' + formatted_eta\n",
        "\n",
        "\n",
        "    def tick_batch(self, batch_loss):\n",
        "        self.local_batches_completed += 1\n",
        "        self.batches_completed += 1\n",
        "\n",
        "        self.batch_loss_list.append(batch_loss)\n",
        "\n",
        "        if self.batches_completed % hp.batch_report_gap == 0:\n",
        "            print('Epoch', f\"{self.epochs_completed / hp.vals_per_epoch : .2f}\",\n",
        "                  'Batch', f\"{self.local_batches_completed:03}\",\n",
        "                  batch_loss, '|',\n",
        "                  self.elapsed_time(), '|',\n",
        "                  self.eta())\n",
        "\n",
        "    def tick_epoch(self, train_loss, test_loss, r2):\n",
        "        self.epochs_completed += 1\n",
        "        self.local_batches_completed = 0\n",
        "\n",
        "        print('### Epoch', f\"{self.epochs_completed / hp.vals_per_epoch : .2f}\", '|',\n",
        "              self.elapsed_time(), '|',\n",
        "              self.eta())\n",
        "        print('train_loss', train_loss, 'test_loss', test_loss, 'r2',r2)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177a1ef7",
      "metadata": {
        "id": "177a1ef7"
      },
      "outputs": [],
      "source": [
        "def get_current_lr(optimizer):\n",
        "    \"\"\"Get the current learning rate from the optimizer.\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def set_learning_rate(optimizer, new_lr):\n",
        "    \"\"\"Set the learning rate for the optimizer.\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af748723",
      "metadata": {
        "id": "af748723"
      },
      "outputs": [],
      "source": [
        "def predict(model):\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, data, _ in eval_dataloader:\n",
        "            prediction = model((images, data)).detach().cpu().numpy()\n",
        "            predictions.append(prediction) # X4,X11,X18,X26,X50,X3112\n",
        "\n",
        "    all_predictions_np = np.concatenate(predictions, axis=0)\n",
        "    df = pd.DataFrame(all_predictions_np, columns=['X4', 'X11', 'X18', 'X26', 'X50', 'X3112'])\n",
        "    df = df[['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']] # format to id,X4,X11,X18,X50,X26,X3112\n",
        "\n",
        "\n",
        "    final_df = pd.concat([eval_img_id, df], axis=1)\n",
        "\n",
        "    filename = 'pred_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '.csv'\n",
        "    final_df.to_csv(filename, index=False)\n",
        "\n",
        "    print(final_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8bb108",
      "metadata": {
        "id": "0d8bb108"
      },
      "source": [
        "# MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ee79b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ee79b0",
        "outputId": "a3c1ed95-a355-4be5-b06a-f0c1b315e991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cur time 02:53:29\n"
          ]
        }
      ],
      "source": [
        "model = SmallBoy()\n",
        "model_to_load = None\n",
        "# model_to_load = os.path.join('data', 'small_boy_untrained.sav')\n",
        "if model_to_load:\n",
        "    if os.path.isfile(model_to_load):\n",
        "        model.load_state_dict(torch.load(model_to_load))\n",
        "        model.eval()\n",
        "        print('loaded:', model_to_load)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=hp.learning_rate)\n",
        "lambda_lr = lambda step: hp.lr_decay ** step\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "bk = bookkeeper(key_lst = ['train_loss', 'test_loss', 'R2'],\n",
        "                total_batches = len(train_dataloader) * hp.num_epochs)\n",
        "bk.reset_timer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d7a921",
      "metadata": {
        "id": "45d7a921"
      },
      "outputs": [],
      "source": [
        "def validate():\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    acc_predictions = [] # hold onto predictions and targets for R2\n",
        "    acc_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, data, target in test_dataloader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            prediction = model((images, data))\n",
        "            test_loss += criterion(prediction, target)\n",
        "\n",
        "            acc_predictions.append(prediction)\n",
        "            acc_targets.append(target)\n",
        "\n",
        "    test_loss = test_loss.item() / len(test_dataloader) / hp.batch_size\n",
        "\n",
        "    acc_targets_cpu = [tensor.cpu().numpy() for tensor in acc_targets]\n",
        "    acc_predictions_cpu = [tensor.cpu().numpy() for tensor in acc_predictions]\n",
        "\n",
        "    r2 = r2_score(np.concatenate(acc_targets_cpu), np.concatenate(acc_predictions_cpu))\n",
        "\n",
        "    return test_loss, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfe3426",
      "metadata": {
        "id": "dcfe3426"
      },
      "outputs": [],
      "source": [
        "epoch_validation_points = []\n",
        "for i in range(hp.vals_per_epoch):\n",
        "    epoch_validation_points.append(len(train_dataloader) * (i+1) // hp.vals_per_epoch)\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = []\n",
        "\n",
        "    for batch_idx, (images, data, target) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        prediction = model((images, data))\n",
        "        loss = criterion(prediction, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bk.tick_batch(loss.item())\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        # bookkeeping 3x PER EPOCH\n",
        "        if (batch_idx+1) in epoch_validation_points:\n",
        "            scheduler.step() # STEPPING SCHEDULER 3x PER EPOCH\n",
        "\n",
        "            test_loss, r2 = validate()\n",
        "            train_loss = np.array(running_loss).mean() / hp.batch_size\n",
        "            bk.append('train_loss', train_loss)\n",
        "            bk.append('test_loss', test_loss)\n",
        "            bk.append('R2', r2)\n",
        "            bk.tick_epoch(train_loss, test_loss, r2)\n",
        "\n",
        "    bk.plot() # plot every epoch\n",
        "\n",
        "    if np.abs(r2) < 100:\n",
        "        torch.save(model.state_dict(), os.path.join('data', f\"small_boy_epoch{epoch}.sav\"))\n",
        "        print('saved', f\"small_boy_epoch{epoch}.sav\", 'r2 = ', r2)\n",
        "        print('lr' , get_current_lr(optimizer))\n",
        "\n",
        "        predict(model)\n",
        "\n",
        "    # for i in range(3 - min(int(np.log10(np.abs(r2))), 3)):\n",
        "    #     scheduler.step()\n",
        "    #     print('additional step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2928bf94",
      "metadata": {
        "id": "2928bf94"
      },
      "outputs": [],
      "source": [
        "def train_model(num_epochs = hp.num_epochs, alr_trained_epochs=0):\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        train_one_epoch(e + 1 + alr_trained_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892795ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "892795ae",
        "outputId": "2c33309a-557f-4137-a773-20d0be71dccf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-69b3cf049c34>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set_learning_rate(optimizer, 0.001* (0.9**5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-7e9955e7e07f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, alr_trained_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malr_trained_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-34bf98c54b9b>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    186\u001b[0m             )\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# set_learning_rate(optimizer, 0.001* (0.9**5))\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d6d0ec6",
      "metadata": {
        "id": "2d6d0ec6"
      },
      "outputs": [],
      "source": [
        "# get_current_lr(optimizer)\n",
        "\n",
        "# torch.save(model.state_dict(), os.path.join('data', f\"small_boy_untrained.sav\"))\n",
        "newdf = predict(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280aa732",
      "metadata": {
        "id": "280aa732"
      },
      "outputs": [],
      "source": [
        "for k in newdf.columns[1:]:\n",
        "    col = newdf[k]\n",
        "    plt.title(k)\n",
        "    plt.hist(col)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02d4766",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c6a32bbc",
      "metadata": {
        "id": "c6a32bbc"
      },
      "source": [
        "todo: make bk pausable\n",
        "\n",
        "and plot in place\n",
        "\n",
        "AND FIX YOUR FUCKING ETA DIPSHIT"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
